<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","version":"8.2.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}};
  </script>
<meta name="description" content="记录Michael Nielsen 的《神经网络与深度学习》学习过程。">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络与深度学习-note">
<meta property="og:url" content="http://example.com/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/index.html">
<meta property="og:site_name" content="lagomerH">
<meta property="og:description" content="记录Michael Nielsen 的《神经网络与深度学习》学习过程。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120081213727.png">
<meta property="og:image" content="http://example.com/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120074657848.png">
<meta property="og:image" content="http://example.com/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120075501785.png">
<meta property="og:image" content="http://example.com/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120092559889.png">
<meta property="og:image" content="http://example.com/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120093803752.png">
<meta property="og:image" content="http://example.com/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120103347029.png">
<meta property="og:image" content="http://example.com/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120115000598.png">
<meta property="og:image" content="http://example.com/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120115226182.png">
<meta property="og:image" content="http://example.com/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120115424951.png">
<meta property="og:image" content="http://example.com/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120115831884.png">
<meta property="og:image" content="http://example.com/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120173437965.png">
<meta property="og:image" content="http://example.com/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120180707163.png">
<meta property="og:image" content="http://example.com/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120180751519.png">
<meta property="og:image" content="http://example.com/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120181846815.png">
<meta property="og:image" content="http://example.com/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120182011643.png">
<meta property="og:image" content="http://example.com/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120182810071.png">
<meta property="og:image" content="http://example.com/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120194421991.png">
<meta property="og:image" content="http://example.com/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120194325646.png">
<meta property="og:image" content="http://example.com/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210121195359475.png">
<meta property="og:image" content="http://example.com/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210122144421179.png">
<meta property="article:published_time" content="2021-01-19T12:30:40.000Z">
<meta property="article:modified_time" content="2021-01-22T11:15:12.302Z">
<meta property="article:author" content="Jiahui Wang">
<meta property="article:tag" content="神经网络">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120081213727.png">


<link rel="canonical" href="http://example.com/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>
<title>神经网络与深度学习-note | lagomerH</title>
  




  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>
  <a target="_blank" rel="noopener" href="https://github.com/lagomerH" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">lagomerH</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">心有猛虎 | 细嗅蔷薇</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
        <li class="menu-item menu-item-schedule"><a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>日程表</a></li>
        <li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>站点地图</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AF%86%E5%88%AB%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97"><span class="nav-number">1.</span> <span class="nav-text">使用神经网络识别手写数字</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%84%9F%E7%9F%A5%E5%99%A8"><span class="nav-number">1.1.</span> <span class="nav-text">感知器</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%84%9F%E7%9F%A5%E5%99%A8%E7%9A%84%E5%AE%9A%E4%B9%89"><span class="nav-number">1.1.1.</span> <span class="nav-text">感知器的定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%80%E5%8C%96%E6%84%9F%E7%9F%A5%E5%99%A8%E7%9A%84%E6%95%B0%E5%AD%A6%E6%8F%8F%E8%BF%B0"><span class="nav-number">1.1.2.</span> <span class="nav-text">简化感知器的数学描述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%84%9F%E7%9F%A5%E5%99%A8%E7%BD%91%E7%BB%9C"><span class="nav-number">1.1.3.</span> <span class="nav-text">感知器网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%84%9F%E7%9F%A5%E5%99%A8%E8%AE%A1%E7%AE%97%E5%9F%BA%E6%9C%AC%E9%80%BB%E8%BE%91%E7%9A%84%E5%8A%9F%E8%83%BD"><span class="nav-number">1.1.4.</span> <span class="nav-text">感知器计算基本逻辑的功能</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#S%E5%9E%8B%E7%A5%9E%E7%BB%8F%E5%85%83"><span class="nav-number">1.2.</span> <span class="nav-text">S型神经元</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95"><span class="nav-number">1.2.1.</span> <span class="nav-text">学习算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%9E%B6%E6%9E%84"><span class="nav-number">1.3.</span> <span class="nav-text">神经网络的架构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E5%88%86%E7%B1%BB%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E7%9A%84%E7%BD%91%E7%BB%9C"><span class="nav-number">1.4.</span> <span class="nav-text">一个简单的分类手写数字的网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E8%BF%9B%E8%A1%8C%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.5.</span> <span class="nav-text">使用梯度下降算法进行学习</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C"><span class="nav-number">2.</span> <span class="nav-text">反向传播算法如何工作</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%94%B9%E8%BF%9B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95"><span class="nav-number">3.</span> <span class="nav-text">改进神经网络的学习方法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%AF%E4%BB%A5%E8%AE%A1%E7%AE%97%E4%BB%BB%E4%BD%95%E5%87%BD%E6%95%B0%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AF%81%E6%98%8E"><span class="nav-number">4.</span> <span class="nav-text">神经网络可以计算任何函数的可视化证明</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%BA%E4%BD%95%E5%BE%88%E9%9A%BE%E8%AE%AD%E7%BB%83"><span class="nav-number">5.</span> <span class="nav-text">深度神经网络为何很难训练</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="nav-number">6.</span> <span class="nav-text">深度学习</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Jiahui Wang"
      src="/images/avatar1.jpg">
  <p class="site-author-name" itemprop="name">Jiahui Wang</p>
  <div class="site-description" itemprop="description">若有恒，何必三更起五更眠；<br>最无益，莫过一日曝十日寒。</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">5</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/lagomerH" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;lagomerH" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:76946112@qq.com" title="E-Mail → mailto:76946112@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar1.jpg">
      <meta itemprop="name" content="Jiahui Wang">
      <meta itemprop="description" content="若有恒，何必三更起五更眠；<br>最无益，莫过一日曝十日寒。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="lagomerH">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          神经网络与深度学习-note
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-01-19 20:30:40" itemprop="dateCreated datePublished" datetime="2021-01-19T20:30:40+08:00">2021-01-19</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-01-22 19:15:12" itemprop="dateModified" datetime="2021-01-22T19:15:12+08:00">2021-01-22</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" itemprop="url" rel="index"><span itemprop="name">神经网络</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p><img src="/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120081213727.png" alt="image-20210120081213727"></p>
<p>记录Michael Nielsen 的《神经网络与深度学习》学习过程。</p>
<a id="more"></a>
<h1 id="使用神经网络识别手写数字"><a href="#使用神经网络识别手写数字" class="headerlink" title="使用神经网络识别手写数字"></a>使用神经网络识别手写数字</h1><p>识别数字主要思想：获取大量的手写数字，常称作训练样本，然后开发出一个可以从这些训练样本中进行学习的系统。即神经网络使用样本来自动推断出识别手写数字的规则。</p>
<p>提高准确性：通过增加训练样本的数量，神经网络可以学到更多关于手写数字的知识。</p>
<h2 id="感知器"><a href="#感知器" class="headerlink" title="感知器"></a>感知器</h2><h3 id="感知器的定义"><a href="#感知器的定义" class="headerlink" title="感知器的定义"></a>感知器的定义</h3><ul>
<li>依据权重来做出决定的设备。</li>
<li>接收几个二进制输入，产生一个二进制输出</li>
</ul>
<p><img src="/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120074657848.png" alt="image-20210120074657848"></p>
<ul>
<li><p>引入<strong>权重</strong>规则，用其表示相应输入对于输出的重要性。</p>
</li>
<li><p><strong>阈(yu)值</strong>是一个实数，一个神经元的参数。神经元的输出(0 or 1)由分配权重后的总和与阈值的大小关系决定。</p>
<p><img src="/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120075501785.png" alt></p>
</li>
</ul>
<h3 id="简化感知器的数学描述"><a href="#简化感知器的数学描述" class="headerlink" title="简化感知器的数学描述"></a>简化感知器的数学描述</h3><ul>
<li><p>第一个变动是用w和x对应权重和输入的向量的点乘。</p>
</li>
<li><p>第二个变动用感知器的偏置b=-threshold代替。</p>
</li>
<li><p><strong>偏置</strong>：一种标识让感知器输出1(激活感知器)有多容易的估算。偏置非常大时输出1是很容易的，偏置非常小时输出1则很困难。</p>
<p><img src="/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120092559889.png" alt></p>
</li>
</ul>
<h3 id="感知器网络"><a href="#感知器网络" class="headerlink" title="感知器网络"></a>感知器网络</h3><ul>
<li><p>第一层感知器——通过权衡输入依据做出三个简单决定。</p>
</li>
<li><p>第二层感知器——权衡第一层的决策结果并作出决定。以这种方式，二层感知器可以比一层感知器做出更复杂和抽象的决策。</p>
</li>
<li><strong>一个感知器只有一个输出</strong>，下图箭头仅仅便于说明一个一个感知器的输出被用于多个感知器。</li>
</ul>
<p><img src="/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120093803752.png" alt="image-20210120093803752" style="zoom: 67%;"></p>
<h3 id="感知器计算基本逻辑的功能"><a href="#感知器计算基本逻辑的功能" class="headerlink" title="感知器计算基本逻辑的功能"></a>感知器计算基本逻辑的功能</h3><ul>
<li><p>运算基础，例如“<strong>与</strong>”，“<strong>或</strong>”和“<strong>与非</strong></p>
</li>
<li><p>与非门感知器：输入00产生1，输入11产生0</p>
</li>
</ul>
<p><img src="/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120103347029.png" alt="image-20210120103347029"></p>
<ul>
<li>与非门是通用运算，以此为基础我们可以用感知器网络来计算任何逻辑功能</li>
<li>构建一个电路将两个二进制数相加</li>
</ul>
<p><img src="/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120115000598.png" alt="image-20210120115000598" style="zoom:80%;"></p>
<ul>
<li>将与非门替换为感知器</li>
</ul>
<p><img src="/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120115226182.png" alt="image-20210120115226182" style="zoom:67%;"></p>
<ul>
<li>最左边感知器的输出被两次作为底部感知器的输入。进而我们可以简单把两条线合并为到一个权重为-4的连接</li>
</ul>
<p><img src="/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120115424951.png" alt="image-20210120115424951" style="zoom: 67%;"></p>
<ul>
<li><strong>输入层</strong>：将<code>x1</code> <code>x2</code>这样的感知器网路左边的浮动变量可以画一层额外的感知器来方便输入编码</li>
</ul>
<p><img src="/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120115831884.png" alt="image-20210120115831884" style="zoom:67%;"></p>
<ul>
<li>输入感知器：可以完全不将其看作感知器，而是简单的定义为输出期望值的特殊单元。</li>
</ul>
<h2 id="S型神经元"><a href="#S型神经元" class="headerlink" title="S型神经元"></a>S型神经元</h2><h3 id="学习算法"><a href="#学习算法" class="headerlink" title="学习算法"></a>学习算法</h3><p>自动调节人工神经元的权重和偏置</p>
<ul>
<li>学习过程：如果对权重(or 偏置)的微小的改动能够仅仅引起输出的微小变化，那么我们可以利用这一事实来修改权重和偏置，让我们的网络表现得像我们想要的那样。假设网络误将一个”9“识别为”8“。我们能够计算出对权重和偏置的微小改动，使网络将其正确识别。我们重复这个工作，反复修改权重和偏置来产生更好的输出。</li>
<li>问题：网络中单个感知器的权重或偏置的微小改动有时候会引起那个感知器的输出完全翻转，如0变到1。这样的翻转可能接下来引起其余网路的行为以极其复杂的方式完全改变。虽然”9“可能别正确分类，网络在其他图像上的行为很可能以一些很难控制的方式被完全改变。这使得逐步修改权重和偏置来让网络接近期望行为变得困难。</li>
</ul>
<p><img src="/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120173437965.png" alt="image-20210120173437965" style="zoom:80%;"></p>
<ul>
<li><p>解决方案：引入一种称为S型神经元的新的人工神经元来克服这个问题。S型神经元和感知器类似，但是被修改为权重和偏置的微小改动只引起输出的微小变化(这对于让神经元网络学习起来是很关键的)。</p>
</li>
<li><p><strong>S型神经元</strong>：有多个输入，这些输入可以取[0,1]中的任意值，而不仅仅是0和1。具有权重和偏置。但输出不是0或者1，而是<code>sigma(w·x+b)</code>,该函数为S型函数(逻辑函数)，定义为：</p>
</li>
</ul>
<p>  <img src="/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120180707163.png" alt="image-20210120180707163"></p>
<p>  代入神经元参数为：</p>
<p>  <img src="/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120180751519.png" alt="image-20210120180751519"></p>
<ul>
<li>S型神经元和感知器关系：当<code>z=w·x+b</code>取很大或者很小时其S型神经元的行为和感知器非常近似。只有<code>z</code>取中间值时与感知模型有较大的偏离。</li>
</ul>
<p><img src="/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120181846815.png" alt="image-20210120181846815" style="zoom:67%;"></p>
<p><img src="/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120182011643.png" alt="image-20210120182011643" style="zoom:67%;"></p>
<ul>
<li>如果<code>sigma</code>函数是个阶跃函数，那么S型神经元将会成为一个感知器。之所以利用<code>sigma</code>函数是为了得到一个平滑的感知器。其<strong>平滑特性</strong>正是解决问题的关键因素，意味着权重和偏置的微小变化会从神经元产生一个微小的输出变化，利用微积分我们可以得到下式：</li>
</ul>
<p><img src="/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120182810071.png" alt="image-20210120182810071" style="zoom:80%;"></p>
<ul>
<li><p>理解:<code>Δoutput</code>是一个反应Δwj和Δb的线性函数，使得选择权重和偏置的微小变化来达到输出的微小变化的运算变得容易。因此，当S型神经元有更多和感知器相同的本质的行为时，计算如何变化权重和偏置来使输出变化会更加容易。</p>
</li>
<li><p>为什么选用上面的<code>sigma</code>函数：对于<code>sigma</code>函数重要的是形状而不是精确形式，选用该函数的目的是简化数学计算，因为指数在求导时有些可爱的属性。</p>
</li>
<li>S型神经元输出的解释：并非只输出0 or 1，但我们可以设定一个约定来解决这个问题，例如约定任何至少为0.5的输出表示为“这是某个数字”</li>
</ul>
<h2 id="神经网络的架构"><a href="#神经网络的架构" class="headerlink" title="神经网络的架构"></a>神经网络的架构</h2><p>假设我们有这样的网络：</p>
<p><img src="/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120194421991.png" alt="image-20210120194421991" style="zoom:80%;"></p>
<ul>
<li><strong>输入层</strong>：网格中最左边的，其中的神经元为<strong>输入神经元</strong>。</li>
<li><strong>输出层</strong>：最右边的，有且只有一个<strong>输出神经元</strong>。</li>
<li><strong>隐藏层</strong>：中间包含既不输入也不输出的神经元的层。</li>
<li><strong>多层感知器</strong> or <strong>MLP</strong>：这种网络尽管是由S型神经元而不是感知器构成，但也称为多层感知器。</li>
</ul>
<p>设计输入输出层通常是比较直接的，隐藏层的设计则堪称一门艺术。特别是，通过一些简单的经验法则来总结隐藏层的设计流程是不可行的。相反，神经网络的研究人员已经为隐藏层开发了许多 设计最优法则。</p>
<ul>
<li><strong>前馈神经网络</strong>：以上一层的输出作为下一层的输入。这意味着网络中是没有回路 的——信息总是向前传播，从不反向反馈。</li>
<li>递归神经网络：反馈环路在其中是可行的，其设计思想是具有休眠前会在一段有限时间内保持激活状态的神经元。</li>
</ul>
<h2 id="一个简单的分类手写数字的网络"><a href="#一个简单的分类手写数字的网络" class="headerlink" title="一个简单的分类手写数字的网络"></a>一个简单的分类手写数字的网络</h2><p><img src="/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210120194325646.png" alt="image-20210120194325646" style="zoom:80%;"></p>
<ul>
<li>网络的输入层包含给输入像素的值进行编码的神经元 ，含有784=28*28个神经元。输入像素是灰度级的，值为1.0表示黑色，值为0.0表示白色，中间数值表示逐渐暗淡的灰色。</li>
<li>网络的隐藏层用n来表示神经元的数量。</li>
<li>网络的输出层包含10个神经元。如果第一个神经元激活，即其输出近似为1，那么表明 网络认为数字是一个0。更确切地说，我们把神经元的输出赋予编号0到9，并计算哪个神经元有最高的激活值。</li>
<li>思考：为什么使用10个输出神经元而不是4个。2^4=16&gt;10完全可以表示10个数字。</li>
<li></li>
</ul>
<h2 id="使用梯度下降算法进行学习"><a href="#使用梯度下降算法进行学习" class="headerlink" title="使用梯度下降算法进行学习"></a>使用梯度下降算法进行学习</h2><ul>
<li><p>训练数据集：用来学习的数据集。</p>
</li>
<li><p><strong>MNIST数据集</strong>：其中包含有数以万计的连带着正确分类器的手写数字的扫描图像。</p>
<p>我们将使用符号x来表示一个训练输入。为了方便，把每个训练输入x看作一个28*28=784维的向量。我们用y=y(x)表示对应的期望输出，这里的y是一个10维向量。例如有一个“6”的训练图像x，那么y(x) = (0,0,0,0,0,0,1,0,0,0)<sup>T</sup> 则是网络的期望输出。T是转置操作，把一个行向量转化成列向量。</p>
</li>
</ul>
<ul>
<li><strong>代价 函数</strong>：我们希望有一个算法，能够让我们找到权重和偏置，以至于网络的输出y(x) 能够拟合所有的训练输入x。为了量化我们如何实现这个目标，我们定义了代价函数：</li>
</ul>
<p><img src="/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210121195359475.png" alt="image-20210121195359475" style="zoom:80%;"></p>
<ul>
<li><code>w</code>：所有的网络中权重的集合；<code>b</code>：所有的偏置；<code>n</code>：训练输入数据的个数；<code>a</code>：当输入为 x 时输出的向量；符号<code>||v||</code>指的是向量v的模。</li>
<li>我们把C称为<strong>二次代价函数</strong> or <strong>均方误差</strong> or <strong>MSE</strong>。</li>
<li><p>该函数是非负的，其值相当小，C(w,b) $\approx$ 0 。精确的说是在对于所有训练输入x，y(x)接近于输出a时。</p>
</li>
<li><p>我们需要使我们的学习算法找到合适的权重和偏置，使得C(w,b) $\approx$ 0，网络就能很好的工作。</p>
</li>
<li>我们采用<strong>梯度下降</strong>的算法来达到这个目的。</li>
<li>思考：为什么不适用正确分类的图片数量作为评量评估网络的能力？<br>被正确分类的图片数量所关于权重和偏置的函数并不是一个平滑函数，大多数情况下，对权重和偏置做出的微小变动完全不会影响被正确分类的图像数量。 </li>
<li>在该过程我们应该将精力集中在<strong>如何最小化代价函数</strong>，可以忽略MNIST，权重和偏置的等前面所提及的东西。</li>
</ul>
<ul>
<li><p><strong>最小化过程——梯度下降算法</strong><br>假设我们最小化函数C(v)，我们用v代替w和b以强调它可能是任意的函数。为了最小化C(v)我们想象C是一个只有两个变量$v_{1}$和$v_{2}$的函数：<br><img src="/2021/01/19/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-note/image-20210122144421179.png" alt="image-20210122144421179" style="zoom:80%;"><br>我们要找到C的全局最小值，对于上图我们一眼就能找到最小值。但通常函数C可能是一个复杂的多元函数，看一下就能找到最小值是不可能的。<br>一种方法是：<strong>微积分来解析最小值</strong>。但是变量过多的话就是噩梦，此时使用微积分显然是不可行的。<br>Luckily，<strong>有一种推导法暗示有一种算法能够得到很好的效果</strong>。</p>
<ul>
<li><p>将函数想象成一个山谷，我们想象有一个小球从山谷的斜坡滚落，日常经验告诉我们这个球最终会滚到谷底。</p>
</li>
<li><p>随机给球体一个起始位置，然后模拟球体滚落到谷底的运动。通过计算C的导数来简单模拟——这些导数能够告诉我们山谷中局部“形状”。</p>
</li>
<li><p>当我们在$v_{1}$和$v_{2}$方向分别将球体移动一个很小的量，微积分告诉我们C将会有如下变化:</p>
</li>
<li><script type="math/tex; mode=display">
\Delta C \approx \frac{\partial C}{\partial v_{1}}\Delta v_{1} + \frac{\partial C}{\partial v_{2}}\Delta v_{2}</script></li>
<li><p>寻找一中选择$\Delta v_{1}$和$\Delta v_{2}$使得$\Delta C$为负的方法。先定义$\Delta v$为$v$的变化向量，$\Delta v \equiv (\Delta v_1, \Delta v_2)^T$，T是转置符号。我们也定义C的梯度为偏导数的向量，$(\frac{\partial C}{\partial v_1},\frac{\partial C}{\partial v_2})^T$.我们用$\bigtriangledown C$表示梯度向量，即：<br>$\bigtriangledown C =(\frac{\partial C}{\partial v_1},\frac{\partial C}{\partial v_2})^T$</p>
</li>
<li><p>$\Delta C \approx \bigtriangledown C \cdot \Delta v$ 。$\bigtriangledown C$把$v$的变化关联为C的变化。——func1</p>
</li>
<li><p>我们选取$\Delta v = -\eta \bigtriangledown C$ ，这里的$\eta$是个很小的正数(<strong>学习速率</strong>)。——func2</p>
</li>
<li><p>结合上述两个式子：$\Delta C \approx - \eta \bigtriangledown C \cdot \bigtriangledown C = -\eta||\bigtriangledown C||^2$，由于$||\bigtriangledown C||^2 \geq 0$，这保证了$\Delta C \leq 0$，即，我们按照上面的选取规则$C$会一直减小，不会增大。因此，我们将上面的方程2用于定义球体在梯度下降算法中的“运动定律”。我们用上面的方程计算$\Delta v$，来移动球体的位置$v$：$v \rightarrow v’ = v-\eta \bigtriangledown C$ 。</p>
</li>
<li><p>不断更新规则来计算下一次移动，反复进行，将持续减小C直到获得一个全局最小值。</p>
</li>
<li><p><strong>总结：</strong>重复计算梯度$\bigtriangledown C$，然后沿着相反方向移动，沿着山谷“滚路”。为了是 梯度下降能够正确运行，我们需要选择足够小的学习速率$\eta$使得func2能够得到很好的近似。如果不这样我们会以$\Delta C &gt; 0$结束；同时，也不能使$\eta$太小，否则会使$\Delta v$变化极小，梯度下降算法就会运行的非常慢。</p>
</li>
<li><p><strong>扩展：</strong>函数$C$具有多个变量时</p>
</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">
\Delta v=(\Delta v_1,...,\Delta v_m)</script><script type="math/tex; mode=display">
\Delta C \approx \bigtriangledown C \cdot \Delta v</script><script type="math/tex; mode=display">
\bigtriangledown C \equiv (\frac{\part C}{\part v_1},...,\frac{\part C}{\part v_m})^T</script><script type="math/tex; mode=display">
\Delta v = -\eta \bigtriangledown C</script><script type="math/tex; mode=display">
v \rightarrow v' = v-\eta \bigtriangledown C</script><ul>
<li><strong>使用梯度下降算法</strong><ul>
<li>“位置”变量用权重和偏置两个变量代替：$w_k,b_l$。</li>
<li>$w_k \rightarrow w_k’= w_k - \eta \frac {\partial C}{\partial w_k}$</li>
<li>$b_l \rightarrow b_l’= b_l - \eta \frac {\partial C}{\partial b_L}$</li>
<li>问题：$C= \frac{1} n \Sigma_x \bigtriangledown C_x$是普及每个训练样本代价$C_x \equiv \frac {||y(x)-a||^2} {2}$的平均值。在实践中，为了计算梯度$\bigtriangledown C$，我们需要为每个训练输入$x$单独的计算梯度值$\bigtriangledown C_x$，然后求平均值，这蒋辉使学习变得相当缓慢。</li>
<li>方案：<strong>随机梯度下降</strong><br>其思想：随机选取小量训练样本来计算$\bigtriangledown C_x$进而估算梯度$\bigtriangledown C$。,<br>随机选取小量的m个训练输入来工作，我们将其标记为$X_1,…,X_m$ ，称为<strong>小批量数据（mini-batcch)</strong>。假设足够大，我们期望$\bigtriangledown C_{X_j}$的平均值大致等于整个$\bigtriangledown C_x$的平均值，即：$\frac{\Sigma^m_{j=1} \bigtriangledown C_{X_j}}{m} \approx \frac{\Sigma_{x} \bigtriangledown C_{x}}{n} = \bigtriangledown C $<br>$\bigtriangledown C \approx \frac{1} {m} \sum ^m_{j=1} \bigtriangledown C_{X}$ </li>
<li>将其明确地和神经网路的学习联系起来，假设$w_k$和$b_l$表示神经网路中 权重和偏置，梯度下降通过随机地选取并训练输入的小批量数据来工作<br>$w_k \rightarrow w_k’ = w_k - \frac{\eta}{m} \sum_j \frac{\partial C_{X_j}}{\partial w_k}$<br>$b_l \rightarrow b_l’ = b_l - \frac{\eta}{m} \sum_j \frac{\partial C_{X_j}}{\partial b_l}$<br>其中两个求和符号实在当前小批量数据中的所有训练样本$X_j$上进行的。然后我们再挑选另一随机选定的小批量数据去训练，直到我们用完了所有的训练输入，这被称为<strong>完成了一个训练迭代期(epoch)</strong>,然后我们就会开始 另一个新的训练迭代期。</li>
</ul>
</li>
</ul>
<h1 id="反向传播算法如何工作"><a href="#反向传播算法如何工作" class="headerlink" title="反向传播算法如何工作"></a>反向传播算法如何工作</h1><h1 id="改进神经网络的学习方法"><a href="#改进神经网络的学习方法" class="headerlink" title="改进神经网络的学习方法"></a>改进神经网络的学习方法</h1><h1 id="神经网络可以计算任何函数的可视化证明"><a href="#神经网络可以计算任何函数的可视化证明" class="headerlink" title="神经网络可以计算任何函数的可视化证明"></a>神经网络可以计算任何函数的可视化证明</h1><h1 id="深度神经网络为何很难训练"><a href="#深度神经网络为何很难训练" class="headerlink" title="深度神经网络为何很难训练"></a>深度神经网络为何很难训练</h1><h1 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h1>
    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag"># 神经网络</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/01/19/hexo-next-blog%E6%90%AD%E5%BB%BA%E5%8F%8A%E4%BD%BF%E7%94%A8/" rel="prev" title="hexo-next blog搭建及使用">
                  <i class="fa fa-chevron-left"></i> hexo-next blog搭建及使用
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/01/19/markdown%E8%AF%AD%E6%B3%95/" rel="next" title="markdown语法">
                  markdown语法 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>







<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jiahui Wang</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>-->

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  






  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'none'
      },
      options: {
        renderActions: {
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>



</body>
</html>
